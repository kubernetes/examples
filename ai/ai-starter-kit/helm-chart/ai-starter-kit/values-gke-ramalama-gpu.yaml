ollama:
  enabled: false # to avoid running two model servers (ollama + ramalama) at the same time

ramalama:
  enabled: true

  nodeSelector:
    cloud.google.com/gke-accelerator: nvidia-l4

  image:
    repository: quay.io/ramalama/cuda
    tag: "latest"
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: "2"
      memory: "4Gi"
      nvidia.com/gpu: 1
    limits:
      cpu: "4"
      memory: "8Gi"
      nvidia.com/gpu: 1

  ## if GPU nodes will be tainted
  # tolerations:
  #   - key: "nvidia.com/gpu"
  #     operator: "Exists"
  #     effect: "NoSchedule"

  command:
    - /bin/sh
    - -c
    - |
      set -e
      echo "GPU info:" && nvidia-smi || true
      echo "Pulling model..." && ramalama pull qwen2.5:1.5b
      echo "Starting server..." && exec ramalama serve qwen2.5:1.5b --port 8080 --device cuda