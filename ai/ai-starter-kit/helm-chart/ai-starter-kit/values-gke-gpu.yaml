jupyterhub:
  singleuser:
    initContainers:
      - name: chown-home-mount-dir
        image: jupyterhub/k8s-singleuser-sample:4.2.0
        securityContext:
          runAsUser: 0
        command: ["chown", "jovyan", "/home/jovyan"]
        volumeMounts:
          - name: home
            mountPath: /home/jovyan
            subPath: jupyterhub_workspace

      - name: model-initializer
        image: jupyterhub/k8s-singleuser-sample:4.2.0
        env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: ai-starter-kit-hf-token-secret
                key: token
        command:
          - /bin/sh
          - -c
          - |
            set -e
            pip install -r /tmp/requirements.txt
            python /tmp/download_models.py "/mnt/jovyan/jupyter_models"
            for f in /tmp/*.ipynb; do
              if [ -f "$f" ]; then
                cp -n "$f" /home/jovyan/
              fi
            done
        volumeMounts:
          - name: home
            mountPath: /home/jovyan
            subPath: jupyterhub_workspace
          - name: init-files
            mountPath: /tmp
            readOnly: true
        resources:
          requests:
            cpu: "2"
            memory: 16Gi
            ephemeral-storage: 10Gi
          limits:
            cpu: "4"
            memory: 32Gi
            ephemeral-storage: 10Gi

    extraResource:
      limits:
        ephemeral-storage: '10G'
        nvidia.com/gpu: 1
      guarantees:
        ephemeral-storage: '10G'
        nvidia.com/gpu: 1

    nodeSelector:
      cloud.google.com/gke-accelerator: nvidia-l4

    image:
      name: jupyter/tensorflow-notebook # - gpu optimzied img
      tag: "latest"

    storage:
      type: static
      static:
        pvcName: "ai-starter-kit-models-cache-pvc"
        subPath: "jupyterhub_workspace"
      capacity: 20Gi
      homeMountPath: /home/jovyan
      extraVolumes:
        - name: init-files
          configMap:
            name: "ai-starter-kit-init-files"
        - name: models-cache
          persistentVolumeClaim:
            claimName: ai-starter-kit-models-cache-only-pvc
      extraVolumeMounts:
        - mountPath: "/mnt/jovyan"
          name: models-cache

    cloudMetadata:
      blockWithIptables: false
    memory:
      limit: 32G
      guarantee: 16G

    profileList:
      - display_name: "GPU Environment"
        description: "Jupyter environment with GPU"
        default: true
        kubespawner_override:
          extra_resource_limits:
            nvidia.com/gpu: "1"
          extra_resource_guarantees:
            nvidia.com/gpu: "1"
          node_selector:
            cloud.google.com/gke-accelerator: nvidia-l4
          #priority_class_name: "high-priority"

  scheduling:
    userScheduler:
      enabled: false

ray-cluster:
  enabled: true
  image:
    tag: 2.41.0-py312-gpu

  head:
    nodeSelector:
      cloud.google.com/gke-accelerator: nvidia-l4
    resources:
      requests:
        cpu: "4"
        memory: "4G"
        ephemeral-storage: 10Gi
        nvidia.com/gpu: 1
      limits:
        cpu: "8"
        memory: "6G"
        ephemeral-storage: 10Gi
        nvidia.com/gpu: 1

  worker:
    nodeSelector:
      cloud.google.com/gke-accelerator: nvidia-l4
    containerEnv:
      - name: PYTHONPATH
        value: "/mnt/ray-storage/libraries"
      - name: TMPDIR
        value: "/mnt/ray-storage/temp"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NVIDIA_VISIBLE_DEVICES
        value: "all"
    resources:
      requests:
        cpu: "4"
        memory: "4G"
        ephemeral-storage: 10Gi
        nvidia.com/gpu: 1
      limits:
        cpu: "8"
        memory: "6G"
        ephemeral-storage: 10Gi
        nvidia.com/gpu: 1
    volumes:
    - name: ray-pvc-storage
      persistentVolumeClaim:
        claimName: "{{ .Release.Name }}-ray-pvc"
    volumeMounts:
      - name: ray-pvc-storage
        mountPath: /mnt/ray-storage
    podSecurityContext:
      fsGroup: 1000

ollama:
  enabled: true
  ollama:
    models:
      pull:
        - gemma3
  persistentVolume:
    enabled: true
    existingClaim: "ai-starter-kit-models-cache-only-pvc"
    subPath: "ollama"

ramalama:
  enabled: true
  nodeSelector:
    cloud.google.com/gke-accelerator: nvidia-l4
  command:
    - /bin/sh
    - -c
    - |
      echo "Autopilot GPU node - starting GPU server..."
      nvidia-smi
      echo "Pulling model..."
      ramalama pull qwen2.5:1.5b
      echo "Starting GPU server..."
      ramalama serve qwen2.5:1.5b --port 8080 --device cuda
  image:
    repository: "quay.io/ramalama/cuda"
    tag: "latest"
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: "2"
      memory: "4Gi"
      nvidia.com/gpu: 1
    limits:
      cpu: "4"
      memory: "8Gi"
      nvidia.com/gpu: 1

rayPvc:
  enabled: true

modelsCachePvc:
  storageClassName: "premium-rwo"
  accessModes:
    - ReadWriteOnce
  size: 20Gi

modelsCacheOnlyPvc:
  enabled: true
  storageClassName: "standard-rwx"
  accessModes:
    - ReadWriteMany
  size: 100Gi

localPersistence:
  enabled: false

genericDevicePlugin:
  enabled: false
