{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079fadd2-200e-4d37-8ae2-be2792e3a24e",
   "metadata": {},
   "source": [
    "### Cell 1 - Install Ollama and verify environment\n",
    "\n",
    "Installs Ollama for local model serving, sets up environment variables, and verifies the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db57cd-fb72-4b10-b0fb-5e9cd5c007b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ollama requests --disable-pip-version-check\n",
    "\n",
    "import os, subprocess, time, json, requests\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['OLLAMA_HOST'] = os.getenv('OLLAMA_HOST', 'http://ai-starter-kit-ollama:11434')\n",
    "MODEL_NAME = \"qwen2.5:1.5b\"\n",
    "MLFLOW_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://ai-starter-kit-mlflow:5000\")\n",
    "\n",
    "OLLAMA_HOST = os.environ['OLLAMA_HOST']\n",
    "\n",
    "print(\"Environment Configuration:\")\n",
    "print(\"Ollama Host:\", OLLAMA_HOST)\n",
    "print(\"Model:      \", MODEL_NAME)\n",
    "print(\"MLflow:     \", MLFLOW_URI)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    r = requests.get(f\"{OLLAMA_HOST}/api/version\", timeout=5)\n",
    "    print(\"Ollama version:\", r.json())\n",
    "except Exception as e:\n",
    "    print(\"Note: Ollama service not running. Starting it in next cell...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe862173-fd9a-41ae-a27b-63875f788024",
   "metadata": {},
   "source": [
    "### Cell 2 - Start Ollama service and pull model\n",
    "\n",
    "Starts the Ollama service if not running, pulls the Qwen 2.5 1.5B model, and verifies it's ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da3e26-6276-48b7-b3ac-c90359df6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "OLLAMA_HOST = os.environ.get('OLLAMA_HOST', 'http://ai-starter-kit-ollama:11434')\n",
    "MODEL_NAME = \"qwen2.5:1.5b\"\n",
    "\n",
    "def check_ollama():\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=2)\n",
    "        return r.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if not check_ollama() and OLLAMA_HOST.startswith(\"http://ai-starter-kit-ollama\"):\n",
    "    print(\"Starting Ollama service...\")\n",
    "    try:\n",
    "        subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        time.sleep(3)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not start Ollama automatically: {e}\")\n",
    "        print(\"Please start Ollama manually with: ollama serve\")\n",
    "\n",
    "if check_ollama():\n",
    "    print(\"Ollama service is running\")\n",
    "    \n",
    "    print(f\"\\nPulling model {MODEL_NAME}...\")\n",
    "    try:\n",
    "        r = requests.get(f\"{OLLAMA_HOST}/api/tags\")\n",
    "        models = r.json().get('models', [])\n",
    "        model_exists = any(m.get('name') == MODEL_NAME for m in models)\n",
    "        \n",
    "        if not model_exists:\n",
    "            pull_data = {\"name\": MODEL_NAME}\n",
    "            r = requests.post(f\"{OLLAMA_HOST}/api/pull\", json=pull_data, stream=True)\n",
    "            for line in r.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = json.loads(line)\n",
    "                        if 'status' in status:\n",
    "                            print(f\"  {status['status']}\", end='\\r')\n",
    "                    except:\n",
    "                        pass\n",
    "            print(f\"\\nModel {MODEL_NAME} pulled successfully\")\n",
    "        else:\n",
    "            print(f\"Model {MODEL_NAME} already available\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model: {e}\")\n",
    "else:\n",
    "    print(\"Warning: Ollama service is not running\")\n",
    "    print(\"Please ensure Ollama is installed and running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111d705-595e-4e65-8479-bdc76191fa31",
   "metadata": {},
   "source": [
    "### Cell 3 - Create OpenAI-compatible API wrapper\n",
    "\n",
    "Sets up a simple FastAPI server that wraps Ollama with an OpenAI-compatible API, including MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea1539-e9ab-460a-9cfc-20a42807f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install fastapi uvicorn mlflow --disable-pip-version-check\n",
    "\n",
    "import os, subprocess, time, json, requests, threading\n",
    "from pathlib import Path\n",
    "\n",
    "api_wrapper_code = '''\n",
    "import os, time, uuid, requests, json\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "\n",
    "USE_MLFLOW = False\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "    if mlflow_uri:\n",
    "        mlflow.set_tracking_uri(mlflow_uri)\n",
    "        mlflow.set_experiment(\"ollama-llm\")\n",
    "        USE_MLFLOW = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "app = FastAPI()\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://ai-starter-kit-ollama:11434\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen2.5:1.5b\")\n",
    "\n",
    "@app.get(\"/v1/healthz\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\", \"model\": MODEL_NAME}\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: Request):\n",
    "    t0 = time.time()\n",
    "    body = await request.json()\n",
    "    \n",
    "    messages = body.get(\"messages\", [])\n",
    "    temperature = body.get(\"temperature\", 0.7)\n",
    "    max_tokens = body.get(\"max_tokens\", 256)\n",
    "    \n",
    "    # Call Ollama API\n",
    "    ollama_payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"num_predict\": max_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(f\"{OLLAMA_HOST}/api/chat\", json=ollama_payload, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        ollama_response = r.json()\n",
    "        \n",
    "        content = ollama_response.get(\"message\", {}).get(\"content\", \"\")\n",
    "        prompt_tokens = len(\" \".join(m.get(\"content\", \"\") for m in messages).split())\n",
    "        completion_tokens = len(content.split())\n",
    "        \n",
    "        if USE_MLFLOW:\n",
    "            try:\n",
    "                with mlflow.start_run():\n",
    "                    mlflow.log_params({\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_tokens\": max_tokens,\n",
    "                        \"model\": MODEL_NAME\n",
    "                    })\n",
    "                    mlflow.log_metrics({\n",
    "                        \"duration_ms\": int((time.time() - t0) * 1000),\n",
    "                        \"prompt_tokens_approx\": prompt_tokens,\n",
    "                        \"completion_tokens_approx\": completion_tokens,\n",
    "                        \"total_tokens_approx\": prompt_tokens + completion_tokens\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            \"id\": \"chatcmpl-\" + uuid.uuid4().hex[:8],\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\"role\": \"assistant\", \"content\": content},\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"total_tokens\": prompt_tokens + completion_tokens\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "with open('/tmp/ollama_wrapper.py', 'w') as f:\n",
    "    f.write(api_wrapper_code)\n",
    "\n",
    "!pkill -f ollama_wrapper.py 2>/dev/null || true\n",
    "\n",
    "env_vars = f\"\"\"\n",
    "export OLLAMA_HOST=\"{os.getenv('OLLAMA_HOST', 'http://ai-starter-kit-ollama:11434')}\"\n",
    "export MODEL_NAME=\"qwen2.5:1.5b\"\n",
    "export MLFLOW_TRACKING_URI=\"{os.getenv('MLFLOW_TRACKING_URI', 'http://ai-starter-kit-mlflow:5000')}\"\n",
    "\"\"\"\n",
    "\n",
    "!echo '{env_vars}' > /tmp/env_vars.sh\n",
    "!bash -c 'source /tmp/env_vars.sh && nohup python /tmp/ollama_wrapper.py > /tmp/wrapper.log 2>&1 &'\n",
    "\n",
    "print(\"Starting API wrapper...\")\n",
    "for i in range(30):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        r = requests.get(\"http://localhost:8000/v1/healthz\", timeout=1)\n",
    "        if r.status_code == 200:\n",
    "            print(\"API Status:\", r.json())\n",
    "            print(f\"\\nOpenAI-compatible API running at: http://localhost:8000/v1\")\n",
    "            print(f\"Health: http://localhost:8000/v1/healthz\")\n",
    "            print(f\"Chat:   http://localhost:8000/v1/chat/completions\")\n",
    "            break\n",
    "    except:\n",
    "        if i % 5 == 0:\n",
    "            print(f\"  Waiting for API to start... ({i}s)\")\n",
    "        continue\n",
    "else:\n",
    "    print(\"\\nAPI wrapper failed to start. Checking logs:\")\n",
    "    !tail -20 /tmp/wrapper.log\n",
    "    print(\"\\nYou can still use direct Ollama API in the next cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411c015-c802-4ca1-81bb-3f4790d9626a",
   "metadata": {},
   "source": [
    "### Cell 4 - Basic client + latency test\n",
    "\n",
    "Tests the OpenAI-compatible API with a simple chat request and measures latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be634e2-a82f-42c9-8e31-57e6868a86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, requests, json\n",
    "\n",
    "USE_WRAPPER = True\n",
    "BASE_URL = \"http://localhost:8000/v1\" if USE_WRAPPER else os.getenv(\"OLLAMA_HOST\", \"http://ai-starter-kit-ollama:11434\")\n",
    "\n",
    "def health():\n",
    "    if USE_WRAPPER:\n",
    "        r = requests.get(f\"{BASE_URL}/healthz\", timeout=10)\n",
    "        print(\"Health:\", r.status_code, r.json())\n",
    "    else:\n",
    "        r = requests.get(f\"{BASE_URL}/api/tags\", timeout=10)\n",
    "        print(\"Health:\", r.status_code, \"Models available:\", len(r.json().get('models', [])))\n",
    "\n",
    "def chat(prompt, temperature=0.4, max_tokens=220):\n",
    "    if USE_WRAPPER:\n",
    "        body = {\n",
    "            \"model\": \"qwen2.5:1.5b\",\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "        }\n",
    "        endpoint = f\"{BASE_URL}/chat/completions\"\n",
    "    else:\n",
    "        body = {\n",
    "            \"model\": \"qwen2.5:1.5b\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"num_predict\": max_tokens\n",
    "            }\n",
    "        }\n",
    "        endpoint = f\"{BASE_URL}/api/chat\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    r = requests.post(endpoint, json=body, timeout=120)\n",
    "    dt = time.time() - t0\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    if USE_WRAPPER:\n",
    "        response = r.json()\n",
    "        content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        usage = response.get(\"usage\", {})\n",
    "    else:\n",
    "        response = r.json()\n",
    "        content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "        usage = {\"total_tokens\": \"estimated: \" + str(len(content.split()) + len(prompt.split()))}\n",
    "    \n",
    "    print(f\"\\nLatency: {dt:.2f}s  | usage: {usage}\")\n",
    "    print(\"\\n---\\n\", content)\n",
    "    return content\n",
    "\n",
    "health()\n",
    "_ = chat(\"Say 'test ok' then give me one short fun fact about llamas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d2756-8949-43e3-8342-71387688e0fa",
   "metadata": {},
   "source": [
    "### Cell 5 - Multi-agent pipeline\n",
    "\n",
    "Implements a simple three-agent workflow (Researcher -> Writer -> Critic) using the local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6713f3-8b60-40b2-ad3c-ebf6db4f66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, json, time\n",
    "\n",
    "BASE_URL = \"http://localhost:8000/v1\"  \n",
    "OLLAMA_DIRECT = os.getenv(\"OLLAMA_HOST\", \"http://ai-starter-kit-ollama:11434\")\n",
    "\n",
    "def call_llm(role_prompt, user_message, temperature=0.4, max_tokens=150, use_wrapper=True):\n",
    "    if use_wrapper:\n",
    "        body = {\n",
    "            \"model\": \"qwen2.5:1.5b\",\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": role_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        }\n",
    "        try:\n",
    "            r = requests.post(f\"{BASE_URL}/chat/completions\", json=body, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    else:\n",
    "        body = {\n",
    "            \"model\": \"qwen2.5:1.5b\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": role_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"num_predict\": max_tokens\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            r = requests.post(f\"{OLLAMA_DIRECT}/api/chat\", json=body, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            return r.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Running Multi-Agent Workflow with Ollama\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "task = \"Research the latest advancements in quantum computing as of 2025.\"\n",
    "\n",
    "try:\n",
    "    r = requests.get(f\"{BASE_URL}/healthz\", timeout=2)\n",
    "    use_wrapper = r.status_code == 200\n",
    "    print(\"Using: OpenAI-compatible wrapper\\n\")\n",
    "except:\n",
    "    use_wrapper = False\n",
    "    print(\"Using: Direct Ollama API\\n\")\n",
    "\n",
    "print(\"1. RESEARCHER:\")\n",
    "print(\"-\" * 40)\n",
    "research_prompt = \"You are a researcher. Provide 3-4 key facts about the topic. Be concise and factual.\"\n",
    "research_notes = call_llm(research_prompt, task, temperature=0.35, max_tokens=140, use_wrapper=use_wrapper)\n",
    "print(research_notes)\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"\\n2. WRITER:\")\n",
    "print(\"-\" * 40)\n",
    "writer_prompt = \"You are a technical writer. Based on the following notes, write a brief report.\"\n",
    "writer_task = f\"Write a report based on these notes:\\n{research_notes}\"\n",
    "report = call_llm(writer_prompt, writer_task, temperature=0.55, max_tokens=220, use_wrapper=use_wrapper)\n",
    "print(report)\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"\\n3. CRITIC/EDITOR:\")\n",
    "print(\"-\" * 40)\n",
    "critic_prompt = \"You are an editor. Review the report and provide a final polished version.\"\n",
    "critic_task = f\"Review and improve this report:\\n{report}\"\n",
    "final_output = call_llm(critic_prompt, critic_task, temperature=0.45, max_tokens=160, use_wrapper=use_wrapper)\n",
    "print(final_output)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Multi-agent workflow complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af596cf-5ba6-42df-a030-61d7a20d6f7b",
   "metadata": {},
   "source": [
    "### Cell 6 - MLFlow: connect to tracking server and list recent runs\n",
    "\n",
    "Connects to MLflow tracking server and displays recent model inference runs with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1b042-04df-4cd0-9099-4cc763ecfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install mlflow==2.14.3 --disable-pip-version-check\n",
    "\n",
    "import os, mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://ai-starter-kit-mlflow:5000\")\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI: {tracking_uri}\")\n",
    "\n",
    "exp_name = \"ollama-llm\"\n",
    "exp = mlflow.set_experiment(exp_name)\n",
    "print(f\"Experiment: {exp.name} (ID: {exp.experiment_id})\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "runs = client.search_runs(\n",
    "    exp.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "if not runs:\n",
    "    print(\"No runs found. Run cells 4 or 5 first to generate inference requests.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(runs)} recent runs:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, run in enumerate(runs, 1):\n",
    "        start_time = datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        duration = run.data.metrics.get('duration_ms', 'N/A')\n",
    "        temp = run.data.params.get('temperature', 'N/A')\n",
    "        max_tokens = run.data.params.get('max_tokens', 'N/A')\n",
    "        total_tokens = run.data.metrics.get('total_tokens_approx', 'N/A')\n",
    "        \n",
    "        print(f\"\\nRun {i}:\")\n",
    "        print(f\"  ID:          {run.info.run_id[:12]}...\")\n",
    "        print(f\"  Time:        {start_time}\")\n",
    "        print(f\"  Status:      {run.info.status}\")\n",
    "        print(f\"  Temperature: {temp}\")\n",
    "        print(f\"  Max Tokens:  {max_tokens}\")\n",
    "        print(f\"  Duration:    {duration} ms\")\n",
    "        print(f\"  Total Tokens: {total_tokens}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY:\")\n",
    "    successful = sum(1 for r in runs if r.info.status == 'FINISHED')\n",
    "    durations = [r.data.metrics.get('duration_ms', 0) for r in runs if r.data.metrics.get('duration_ms')]\n",
    "    avg_duration = sum(durations) / len(durations) if durations else 0\n",
    "    \n",
    "    print(f\"  Total Runs: {len(runs)}\")\n",
    "    print(f\"  Successful: {successful}\")\n",
    "    print(f\"  Failed: {len(runs) - successful}\")\n",
    "    print(f\"  Avg Duration: {avg_duration:.1f} ms\" if avg_duration else \"  Avg Duration: N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MLflow verification complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
