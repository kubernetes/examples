{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079fadd2-200e-4d37-8ae2-be2792e3a24e",
   "metadata": {},
   "source": [
    "### Cell 1 - Install RamaLama and verify environment\n",
    "\n",
    "Installs RamaLama for local model serving, sets up environment variables, and verifies the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db57cd-fb72-4b10-b0fb-5e9cd5c007b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install requests --disable-pip-version-check\n",
    "\n",
    "import os, time, json, requests\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['RAMALAMA_HOST'] = 'http://ai-starter-kit-ramalama:8080'\n",
    "MODEL_NAME = \"qwen2.5:1.5b\"\n",
    "MLFLOW_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://ai-starter-kit-mlflow:5000\")\n",
    "\n",
    "RAMALAMA_HOST = os.environ['RAMALAMA_HOST']\n",
    "\n",
    "print(\"Environment Configuration:\")\n",
    "print(\"RamaLama Host:\", RAMALAMA_HOST)\n",
    "print(\"Model:      \", MODEL_NAME)\n",
    "print(\"MLflow:     \", MLFLOW_URI)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    r = requests.get(f\"{RAMALAMA_HOST}/v1/models\", timeout=5)\n",
    "    print(\"RamaLama models:\", r.json())\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to RamaLama: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe862173-fd9a-41ae-a27b-63875f788024",
   "metadata": {},
   "source": [
    "### Cell 2 - Start RamaLama service and pull model\n",
    "\n",
    "Starts the RamaLama service if not running, pulls the Qwen 2.5 1.5B model, and verifies it's ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da3e26-6276-48b7-b3ac-c90359df6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, json\n",
    "\n",
    "RAMALAMA_HOST = os.environ.get('RAMALAMA_HOST', 'http://ai-starter-kit-ramalama:8080')\n",
    "MODEL_NAME = \"qwen2.5:1.5b\"\n",
    "\n",
    "def check_ramalama():\n",
    "    try:\n",
    "        r = requests.get(f\"{RAMALAMA_HOST}/v1/models\", timeout=2)\n",
    "        return r.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if check_ramalama():\n",
    "    print(\"RamaLama service is running\")\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(f\"{RAMALAMA_HOST}/v1/models\")\n",
    "        models = r.json().get('data', [])\n",
    "        model_exists = any(m.get('id') == MODEL_NAME for m in models) \n",
    "        if model_exists:\n",
    "            print(f\"Model {MODEL_NAME} already available\")\n",
    "        else:\n",
    "            print(f\"Model {MODEL_NAME} not found; ensure it's pulled in the deployment\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking model: {e}\")\n",
    "else:\n",
    "    print(\"Warning: RamaLama service is not running\")\n",
    "    print(\"Please ensure the deployment is healthy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111d705-595e-4e65-8479-bdc76191fa31",
   "metadata": {},
   "source": [
    "### Cell 3 - Create OpenAI-compatible API wrapper\n",
    "\n",
    "Sets up a simple FastAPI server that wraps RamaLama with an OpenAI-compatible API, including MLflow tracking. Since RamaLama already provides OpenAI compatibility, this acts as a proxy with logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea1539-e9ab-460a-9cfc-20a42807f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install fastapi uvicorn mlflow --disable-pip-version-check\n",
    "\n",
    "import os, threading, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "api_wrapper_code = '''\n",
    "import os, time, uuid, requests, json\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "\n",
    "USE_MLFLOW = False\n",
    "try:\n",
    "    import mlflow\n",
    "    mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "    if mlflow_uri:\n",
    "        mlflow.set_tracking_uri(mlflow_uri)\n",
    "        mlflow.set_experiment(\"ramalama-llm\")\n",
    "        USE_MLFLOW = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "app = FastAPI()\n",
    "RAMALAMA_HOST = os.getenv(\"RAMALAMA_HOST\", \"http://127.0.0.1:8080\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen2.5:1.5b\")\n",
    "\n",
    "@app.get(\"/v1/healthz\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\", \"model\": MODEL_NAME}\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: Request):\n",
    "    t0 = time.time()\n",
    "    body = await request.json()\n",
    "    \n",
    "    messages = body.get(\"messages\", [])\n",
    "    temperature = body.get(\"temperature\", 0.7)\n",
    "    max_tokens = body.get(\"max_tokens\", 256)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(f\"{RAMALAMA_HOST}/v1/chat/completions\", json=payload, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        response = r.json()\n",
    "        \n",
    "        content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        usage = response.get(\"usage\", {})\n",
    "        prompt_tokens = usage.get(\"prompt_tokens\", len(\" \".join(m.get(\"content\", \"\") for m in messages).split()))\n",
    "        completion_tokens = usage.get(\"completion_tokens\", len(content.split()))\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        \n",
    "        if USE_MLFLOW:\n",
    "            try:\n",
    "                with mlflow.start_run():\n",
    "                    mlflow.log_params({\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_tokens\": max_tokens,\n",
    "                        \"model\": MODEL_NAME\n",
    "                    })\n",
    "                    mlflow.log_metrics({\n",
    "                        \"duration_ms\": int((time.time() - t0) * 1000),\n",
    "                        \"prompt_tokens\": prompt_tokens,\n",
    "                        \"completion_tokens\": completion_tokens,\n",
    "                        \"total_tokens\": total_tokens\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            \"id\": \"chatcmpl-\" + uuid.uuid4().hex[:8],\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\"role\": \"assistant\", \"content\": content},\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"total_tokens\": total_tokens\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "with open('/tmp/ramalama_wrapper.py', 'w') as f:\n",
    "    f.write(api_wrapper_code)\n",
    "\n",
    "def run_api():\n",
    "    subprocess.run([\"python\", \"/tmp/ramalama_wrapper.py\"], capture_output=True)\n",
    "\n",
    "import subprocess\n",
    "api_process = subprocess.Popen(\n",
    "    [\"python\", \"/tmp/ramalama_wrapper.py\"],\n",
    "    env={**os.environ, \n",
    "         \"RAMALAMA_HOST\": os.getenv(\"RAMALAMA_HOST\", \"http://127.0.0.1:8080\"),\n",
    "         \"MODEL_NAME\": MODEL_NAME,\n",
    "         \"MLFLOW_TRACKING_URI\": MLFLOW_URI},\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "try:\n",
    "    r = requests.get(f\"{API_URL}/v1/healthz\", timeout=5)\n",
    "    print(\"API Status:\", r.json())\n",
    "    print(f\"\\nOpenAI-compatible API running at: {API_URL}/v1\")\n",
    "    print(f\"Health: {API_URL}/v1/healthz\")\n",
    "    print(f\"Chat:   {API_URL}/v1/chat/completions\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: API wrapper not responding: {e}\")\n",
    "    print(\"You may need to run the wrapper manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411c015-c802-4ca1-81bb-3f4790d9626a",
   "metadata": {},
   "source": [
    "### Cell 4 - Basic client + latency test\n",
    "\n",
    "Tests the OpenAI-compatible API with a simple chat request and measures latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be634e2-a82f-42c9-8e31-57e6868a86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, requests, json\n",
    "\n",
    "USE_WRAPPER = True\n",
    "BASE_URL = \"http://localhost:8000/v1\" if USE_WRAPPER else os.getenv(\"RAMALAMA_HOST\", \"http://127.0.0.1:8080\")\n",
    "\n",
    "def health():\n",
    "    if USE_WRAPPER:\n",
    "        r = requests.get(f\"{BASE_URL}/healthz\", timeout=10)\n",
    "        print(\"Health:\", r.status_code, r.json())\n",
    "    else:\n",
    "        r = requests.get(f\"{BASE_URL}/v1/models\", timeout=10)\n",
    "        print(\"Health:\", r.status_code, \"Models available:\", r.json().get('data', []))\n",
    "\n",
    "def chat(prompt, temperature=0.4, max_tokens=220):\n",
    "    body = {\n",
    "        \"model\": \"qwen2.5:1.5b\",\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    endpoint = f\"{BASE_URL}/chat/completions\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "    r = requests.post(endpoint, json=body, timeout=120)\n",
    "    dt = time.time() - t0\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    response = r.json()\n",
    "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    usage = response.get(\"usage\", {\"total_tokens\": \"estimated: \" + str(len(content.split()) + len(prompt.split()))})\n",
    "    \n",
    "    print(f\"\\nLatency: {dt:.2f}s  | usage: {usage}\")\n",
    "    print(\"\\n---\\n\", content)\n",
    "    return content\n",
    "\n",
    "health()\n",
    "_ = chat(\"Say 'test ok' then give me one short fun fact about llamas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d2756-8949-43e3-8342-71387688e0fa",
   "metadata": {},
   "source": [
    "### Cell 5 - Multi-agent pipeline\n",
    "\n",
    "Implements a simple three-agent workflow (Researcher -> Writer -> Critic) using the local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6713f3-8b60-40b2-ad3c-ebf6db4f66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, json, time\n",
    "\n",
    "BASE_URL = \"http://localhost:8000/v1\"  \n",
    "RAMALAMA_DIRECT = os.getenv(\"RAMALAMA_HOST\", \"http://127.0.0.1:8080\")\n",
    "\n",
    "def call_llm(role_prompt, user_message, temperature=0.4, max_tokens=150, use_wrapper=True):\n",
    "    body = {\n",
    "        \"model\": \"qwen2.5:1.5b\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": role_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    if use_wrapper:\n",
    "        endpoint = f\"{BASE_URL}/chat/completions\"\n",
    "    else:\n",
    "        endpoint = f\"{RAMALAMA_DIRECT}/v1/chat/completions\"\n",
    "    try:\n",
    "        r = requests.post(endpoint, json=body, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        response = r.json()\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Running Multi-Agent Workflow with RamaLama\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "task = \"Research the latest advancements in quantum computing as of 2025.\"\n",
    "\n",
    "try:\n",
    "    r = requests.get(f\"{BASE_URL}/healthz\", timeout=2)\n",
    "    use_wrapper = r.status_code == 200\n",
    "    print(\"Using: OpenAI-compatible wrapper\\n\")\n",
    "except:\n",
    "    use_wrapper = False\n",
    "    print(\"Using: Direct RamaLama API\\n\")\n",
    "\n",
    "print(\"RESEARCHER:\")\n",
    "print(\"-\" * 40)\n",
    "research_prompt = \"You are a researcher. Provide 3-4 key facts about the topic. Be concise and factual.\"\n",
    "research_notes = call_llm(research_prompt, task, temperature=0.35, max_tokens=140, use_wrapper=use_wrapper)\n",
    "print(research_notes)\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"\\nWRITER:\")\n",
    "print(\"-\" * 40)\n",
    "writer_prompt = \"You are a technical writer. Based on the following notes, write a brief report.\"\n",
    "writer_task = f\"Write a report based on these notes:\\n{research_notes}\"\n",
    "report = call_llm(writer_prompt, writer_task, temperature=0.55, max_tokens=220, use_wrapper=use_wrapper)\n",
    "print(report)\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"\\nCRITIC/EDITOR:\")\n",
    "print(\"-\" * 40)\n",
    "critic_prompt = \"You are an editor. Review the report and provide a final polished version.\"\n",
    "critic_task = f\"Review and improve this report:\\n{report}\"\n",
    "final_output = call_llm(critic_prompt, critic_task, temperature=0.45, max_tokens=160, use_wrapper=use_wrapper)\n",
    "print(final_output)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Multi-agent workflow complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af596cf-5ba6-42df-a030-61d7a20d6f7b",
   "metadata": {},
   "source": [
    "### Cell 6 - MLFlow: connect to tracking server and list recent runs\n",
    "\n",
    "Connects to MLflow tracking server and displays recent model inference runs with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1b042-04df-4cd0-9099-4cc763ecfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install mlflow==2.14.3 --disable-pip-version-check\n",
    "\n",
    "import os, mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://ai-starter-kit-mlflow:5000\")\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(f\"MLflow Tracking URI: {tracking_uri}\")\n",
    "\n",
    "exp_name = \"ramalama-llm\"\n",
    "exp = mlflow.set_experiment(exp_name)\n",
    "print(f\"Experiment: {exp.name} (ID: {exp.experiment_id})\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "runs = client.search_runs(\n",
    "    exp.experiment_id,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "if not runs:\n",
    "    print(\"No runs found. Run cells 4 or 5 first to generate inference requests.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(runs)} recent runs:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, run in enumerate(runs, 1):\n",
    "        start_time = datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        duration = run.data.metrics.get('duration_ms', 'N/A')\n",
    "        temp = run.data.params.get('temperature', 'N/A')\n",
    "        max_tokens = run.data.params.get('max_tokens', 'N/A')\n",
    "        total_tokens = run.data.metrics.get('total_tokens', 'N/A')\n",
    "        \n",
    "        print(f\"\\nRun {i}:\")\n",
    "        print(f\"  ID:          {run.info.run_id[:12]}...\")\n",
    "        print(f\"  Time:        {start_time}\")\n",
    "        print(f\"  Status:      {run.info.status}\")\n",
    "        print(f\"  Temperature: {temp}\")\n",
    "        print(f\"  Max Tokens:  {max_tokens}\")\n",
    "        print(f\"  Duration:    {duration} ms\")\n",
    "        print(f\"  Total Tokens: {total_tokens}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY:\")\n",
    "    successful = sum(1 for r in runs if r.info.status == 'FINISHED')\n",
    "    durations = [r.data.metrics.get('duration_ms', 0) for r in runs if r.data.metrics.get('duration_ms')]\n",
    "    avg_duration = sum(durations) / len(durations) if durations else 0\n",
    "    \n",
    "    print(f\"  Total Runs: {len(runs)}\")\n",
    "    print(f\"  Successful: {successful}\")\n",
    "    print(f\"  Failed: {len(runs) - successful}\")\n",
    "    print(f\"  Avg Duration: {avg_duration:.1f} ms\" if avg_duration else \"  Avg Duration: N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MLflow verification complete\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
